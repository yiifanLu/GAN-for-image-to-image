import functools
import logging
import torch
import torch.nn as nn
from torch.nn import init

# =============================================================================
# import models.arch.discriminator_dispatcher as discriminator_dispatcher
# import models.arch.generator_cfm as generator_cfm
# import models.arch.generator_resnet as generator_resnet
# import models.arch.simple_generator_cfm as simple_generator_cfm
# 
# #ablation
# import models.arch.generator_cfm_0_limit_scale_no_negative as generator_cfm_scale_no_neg
# import models.arch.generator_cfm_0_limit_scale as generator_cfm_limit_scale 
# import models.arch.generator_cfm_0_limit_shift as generator_cfm_limit_shift 
# =============================================================================

import models.arch.generator_unet as generator_unet
import models.arch.discriminator_patch as discriminator_patch
logger = logging.getLogger('base')


####################
# initialize
####################

def weights_init_normal(m, std=0.02):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        init.normal_(m.weight.data, 0.0, std)
        if m.bias is not None:
            m.bias.data.zero_()
    elif classname.find('Linear') != -1:
        init.normal_(m.weight.data, 0.0, std)
        if m.bias is not None:
            m.bias.data.zero_()
    elif classname.find('BatchNorm2d') != -1:
        init.normal_(m.weight.data, 1.0, std)  # BN also uses norm
        init.constant_(m.bias.data, 0.0)


def weights_init_kaiming(m, scale=1):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')
        m.weight.data *= scale
        if m.bias is not None:
            m.bias.data.zero_()
    elif classname.find('Linear') != -1:
        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')
        m.weight.data *= scale
        if m.bias is not None:
            m.bias.data.zero_()
    elif classname.find('BatchNorm2d') != -1:
        init.constant_(m.weight.data, 1.0)
        init.constant_(m.bias.data, 0.0)



def init_weights(net, init_type='kaiming', scale=1, std=0.02):
    # scale for 'kaiming', std for 'normal'.
    logger.info('Initialization method [{:s}]'.format(init_type))
    if init_type == 'normal':
        weights_init_normal_ = functools.partial(weights_init_normal, std=std)
        net.apply(weights_init_normal_)
    elif init_type == 'kaiming':
        weights_init_kaiming_ = functools.partial(weights_init_kaiming, scale=scale)
        net.apply(weights_init_kaiming_)
    else:
        raise NotImplementedError('initialization method [{:s}] not implemented'.format(init_type))


####################
# define network
####################

# Generator
def define_G(opt):
    gpu_ids = opt['gpu_ids']
    opt_net = opt['network_G']
    which_model = opt_net['which_model_G']
    if which_model == 'U-net':  
        netG = generator_unet.UNET_Network(input_nc=12, output_nc=3, num_downs=8, ngf=64,  use_dropout=True) 
# =============================================================================
#     if which_model == 'generator_cfm':  
#         netG = generator_cfm.CFM_Network() 
#     elif which_model == 'generator_resnet_no_condition':
#         netG = generator_resnet.ResNet_no_Condition() 
#     elif which_model == "generator_resnet_condition":
#         netG = generator_resnet.ResNet_Condition()       
#     elif which_model == 'simple_generator_cfm': 
#         netG = simple_generator_cfm.CFM_Network() 
#     elif which_model == "generator_cfm_limit_scale":
#         netG = generator_cfm_limit_scale.CFM_Network()
#     elif which_model == "generator_cfm_limit_shift":
#         netG = generator_cfm_limit_shift.CFM_Network()
#     elif which_model == "generator_cfm_scale_no_neg":
#         netG = generator_cfm_scale_no_neg.CFM_Network()    
# =============================================================================
    else:
        raise NotImplementedError('Generator model [{:s}] not recognized'.format(which_model))

    if opt['is_train']:
        init_weights(netG, init_type='normal', scale=0.1)
    if gpu_ids:
        assert torch.cuda.is_available()
        netG = nn.DataParallel(netG)
    return netG


# Discriminator
def define_D(opt):
    gpu_ids = opt['gpu_ids']
    opt_net = opt['network_D']
    which_model = opt_net['which_model_D']
    if which_model == 'PatchGAN':
        netD=discriminator_patch.discriminator_patch(input_nc=15, ndf=64, n_layers=3)
# =============================================================================
#     if which_model == 'discriminator_vgg_128':
#         netD = discriminator_dispatcher.Discriminator_VGG_128(in_nc=opt_net['in_nc'], base_nf=opt_net['nf'], \
#             norm_type=opt_net['norm_type'], mode=opt_net['mode'], act_type=opt_net['act_type'])
#     elif which_model == 'discriminator_vgg_96':
#         netD = discriminator_dispatcher.Discriminator_VGG_96(in_nc=opt_net['in_nc'], base_nf=opt_net['nf'], \
#             norm_type=opt_net['norm_type'], mode=opt_net['mode'], act_type=opt_net['act_type'])
#     elif which_model == 'discriminator_vgg_192':
#         netD = discriminator_dispatcher.Discriminator_VGG_192(in_nc=opt_net['in_nc'], base_nf=opt_net['nf'], \
#             norm_type=opt_net['norm_type'], mode=opt_net['mode'], act_type=opt_net['act_type'])
# =============================================================================
    else:
        raise NotImplementedError('Discriminator model [{:s}] not recognized'.format(which_model))

    init_weights(netD, init_type='normal', scale=1)
    if gpu_ids:
        netD = nn.DataParallel(netD)
    return netD


def define_F(opt, use_bn=False):
    gpu_ids = opt['gpu_ids']
    device = torch.device('cuda' if gpu_ids else 'cpu')
    # pytorch pretrained VGG19-54, before ReLU.
    if use_bn:
        feature_layer = 49
    else:
        feature_layer = 34
    netF = discriminator_dispatcher.VGGFeatureExtractor(feature_layer=feature_layer, use_bn=use_bn, \
        use_input_norm=True, device=device)
    if gpu_ids:
        netF = nn.DataParallel(netF)
    netF.eval()  # No need to train
    return netF
